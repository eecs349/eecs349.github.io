<!DOCTYPE html>
<!-- saved from url=(0050)http://getbootstrap.com/examples/starter-template/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="icon" href="img/beer.ico">

    <title>EECS 349 Final Project</title>

    <!-- Bootstrap core CSS -->
    <link href="http://getbootstrap.com/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="http://getbootstrap.com/examples/starter-template/starter-template.css" rel="stylesheet">

    <link href="./css/custom.css" rel="stylesheet">
    <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
    <script src="./js/ie-emulation-modes-warning.js"></script>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>

    <nav class="navbar navbar-default navbar-fixed-top">
        <div id="header-custom" class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" dampta-target="#bs-exale-navbar-collapse-1">
                    <span class="sr-only"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand page-scroll" href="#">What's Cooking</a>
            </div>
            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li class="hidden">
                        <a href="#page-top"></a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#summary">Summary</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#report">Report</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#conclusion">Conclusion</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container-fluid -->
    </nav>

     <header>
        <div class="container">
            <div class="intro-text">
                <center><div class="intro-heading"></div></center>
                <center><div class="intro-heading"></div></center>
                <center><div class="intro-heading"></div></center>
                <center><div class="intro-lead-in-team">Manshan Lin, Xin Tong, Jianping Zhang, Fan Hu
<br>EECS 349 at Northwestern University<br>
<a href="mailto: jianpingzhang2018@u.northwestern.edu" id="contact">jianpingzhang2018@u.northwestern.edu</a><br>
<a href="mailto:  xintong2018@u.northwestern.edu" id="contact">xintong2018@u.northwestern.edu</a></div></center>
            </div>
        </div>
    </header>

    <section id="summary">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Summary</h2>
                </div>
            </div>
            <div class="row text-left" id="width">
                <div class="col-lg-12">
                        <p>
                        Different countries and regions have different cooking habits and corresponding recipes. So Our task is to build a classifier to determine the cuisine of a recipe based on its ingredients. The reason why we attach great importance to this task is that this classifier can serve as important reference for customers who go to unfamiliar restaurant and order unfamiliar food. Moreover, such task can cultivate our abilities to apply different machine learning algorithms into specific real world problems and of course, to use effective data-preprocessing techniques to deal with the raw data.</p>
                        <p>
                        We applied different algorithms including decision tree, RandomForest, logistic regression (L1 & L2), SVM and KNN, adjusted the parameters of each algorithm, and tried two methods to reduce dimension—— eliminating all parents-children structure, and PCA. We used the feature “ingredient” to predict the feature “cuisine”.</p>
                    <figure id=figure>
                            <img src="img/word_cloud_01.png" id="graph">
                            <center><figcaption>Word Cloud</figcaption></center>
                    </figure>
                    <figure id=figure>
                            <img src="img/mostly_used_ingredients_chinese.jpeg" id="graph">
                            <center><figcaption>Top most used ingredients in "Chinese" Cuisine</figcaption></center>
                    </figure>


                </div>
            </div>
        </div>
    </section>
    <section id="report">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Report</h2>
                </div>
            </div>
            <div class="row text-left" id="width">
                <div class="col-lg-12" >
                    <h2>Data preprocessing</h2>
                    <p>
                        We obtain the original data from kaggle. They provide 39774 training samples and 9944 test samples. Each training sample comes with a unique id, a “cuisine” label referring its category and several ingredients. Basically, cuisine is our training target and ingredients are our inputs.
                    </p>
                    <h2>Feature Engineering</h2>
                    <p>
                        We build the bag of words which is used to count the frequency of the occurrence of each ingredients and we build the ingredients dictionary which regards each ingredient as a vector and represents the occurrence by 0/1 code with the use of one-hot encoding.

                        The result of the word bags is nearly 6000 ingredients and the occurrence frequency is ranging from 1 to 18000. The result of the ingredients Vector is nearly 6000 features and 40000 records. The attribute value of each feature is 0/1 code. We also applied TF*IDF Method to deal with our dataset, and trained it on SVM model.
                    </p>
                    <h2>Model Training</h2>
                    <p>
                        We applied different algorithms including decision tree, RandomForest, logistic regression (L1 & L2), SVM and KNN, adjusted the parameters of each algorithm, and tried different methods to reduce dimension, respectively get best accuracy of 62.1%, 73.1%, 78.4% & 78.4%, 81.1%, and 51.6. (We submitted our test result to get corresponding accuracy from Kaggle.)
                    </p>
                    <p>
                        For the decision tree and RandomForest, we obtained the accuracy of 62.1% and 73.1% on the testing set. By calculating the information gain, we got the features’ importance list which is an important reference for the later re-processing data.
                    </p>
                    <p>
                        For the logistic regression, we used one versus rest logistic regression to build one regression model for each cuisine, in order to observe weights and display correlated ingredients for each cuisine. Both L1 and L2 version logistic regression are tried and the accuracy on testing set are both around 78%, while l2 version performs slightly better. So we choose L2 penalty and 1.2 penalty weight. We also use parameters of logistic regression to show each ingredient importance for each cuisine.
                    </p>
                    <p>
                        For the SVM, we chose One-or-Rest support vector classifier method to train the model. To determine the weight of ingredients, we chose the term frequency–inverse document frequency (TF*IDF) method, fully considering the functions of term frequency, term count, and inverse document frequency. Eventually we got 81% accuracy on the testing set.
                    </p>
                    <p>
                        For the KNN, by calculating the distance based on the ingredients vector, we can determine the cuisine according to its nearest neighbor. The accuracy is around 59% on the testing set.

                    </p>

                    <h2>Reduce Dimension</h2>
                    <p>
                        At first, we tried to use top frequency feature dimension reduction strategy. After our tests on dataset, we found out that reducing the whole dataset to the top 3000 features can largely reduce computation complexity with slightly loss of accuracy.
                    </p>
                    <p>
                        Then, since there are some ingredients which can be seen as parents and children. (e.g. “tomato” and “green tomato”), we did some text processing work, trying to include all children situation to their parents. So we got another data set without the parent-children structure. We re-trained our model using the data which has eliminated the parent-children structure and deciding which data set to use based on the test accuracy.
                    </p>
                    <p>
                        What’s more, we also tried PCA dimension reduction strategy to reduce dimension, and retrained our model to identify whether we need to use dimension reduction technique. The detailed output are shown on the following table. We can see that based on our dataset, neither PCA dimension reduction strategy or eliminating the parent-children structure have a significant effect on the accuracy. Only Random Forest and Decision Tree have a slightly improvement on accuracy eliminating the parent-children structure. So We decided not use the two dimension reducing methods. But, this attempt is still necessary and worthwhile.
                    </p>
                    <table>
                        <tr>
                            <th>Accuracy \ Dataset</th>
                            <th>using all features</th>
                            <th>using top 2000 features</th>
                            <th>using top 3000 features</th>
                            <th>using pca on 3000 features</th>
                            <th>eliminating parent struc. on 3000 features</th>
                        </tr>
                        <tr>
                            <td>Logistic Regression</td>
                            <td>78.378%</td>
                            <td>77.685%</td>
                            <td>78.258%</td>
                            <td>78.231%</td>
                            <td>74.202%</td>
                        </tr>
                        <tr>
                            <td>Random Forest</td>
                            <td>73.079%</td>
                            <td>72.576%</td>
                            <td>72.976%</td>
                            <td>58.286%</td>
                            <td>73.561%</td>
                        </tr>
                        <tr>
                            <td>Decision Tree</td>
                            <td>62.131%</td>
                            <td>60.250%</td>
                            <td>61.002%</td>
                            <td>39.732%</td>
                            <td>61.253%</td>
                        </tr>                       
                        <tr>
                            <td>KNN</td>
                            <td>51.627%</td>
                            <td></td>
                            <td></td>
                            <td></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td>SVM</td>
                            <td>81.109%</td>
                            <td>81.109%</td>
                            <td>81.109%</td>
                            <td>69.703%</td>
                            <td>66.512%</td>
                        </tr>

                        <tr>
                            <td>AdaBoost (Decision Tree by default)</td>
                            <td></td>
                            <td></td>
                            <td>53.435%</td>
                            <td>40.238%</td>
                            <td>57.492%</td>


                        </tr>
                    </table>
                </div>
            </div>
        </div>
    </section>
    
 <section id="conclusion">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Conclusion</h2>
                </div>
            </div>
            <div class="row text-left" id="width">
                <div class="col-lg-12">
                        <p style="color:#FAB934;">
                            Analyzing our results, we found that nearly for all algorithms, the more data we use, the better result we will get. Reduce dimension is only necessary if our computation power is limited, otherwise we suggest to use all the data.
                        </p>

                        <p style="color:#FAB934;">
                            We also found that PCA will decrease accuracy, especially for SVM, decision tree and random forest, which is easy to understand. For SVM, it will try to map data from low dimension to high dimension to get a full split on these data, while PCA will try to reduce dimension. SVM and PCA are kind of like opposite processe. Also, Decision Tree and Random Forest will do better with high dimension data since more specific data will generate more accurate result. It's no wonder that PCA will largely decrease accuracy for SVM, DT and Random Forest. We also notice that eliminate Parent-Children structure will slightly improve Tree algorithms, which can be attribute to this would generate some more general rules and avoid overfitting. 
                        </p>

                        <p style="color:#FAB934;">
                            There are some future work we can do if we have more time. For example, we can eliminate some basic ingredients such as "salt" and add more weights to some unique ingredients such as soy sauce for asia foods. We could visualize the weights of each ingredients in Decision Trees and assign these weights to logistic regression manually, we believe these processing may bring some interesting results.
                            <br><br>
                        </p>
                        <center><a href="http://eecs349.github.io/downloads/codes.zip" id="download">Download our codes</a></center>
                        <center><a href="http://eecs349.github.io/downloads/349_final_report.pdf" id="download">Download our report as a PDF</a></center>
                </div>
            </div>
        </div>
    </section>

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="./js/jquery.min.js"></script>
    <script src="./js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="./js/ie10-viewport-bug-workaround.js"></script>
  

</body></html>
